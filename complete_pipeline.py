"""
üîÑ COMPLETE DATA PIPELINE
Excel/CSV ‚Üí Geocoding ‚Üí Spatial Filtering ‚Üí Supabase

Workflow:
1. Load Excel/CSV
3. Filter points within district boundaries
4. Upload to Supabase
"""

import pandas as pd
import geopandas as gpd
import requests
from supabase import create_client, Client
import time
import json
import os
from dotenv import load_dotenv
import osmnx as ox
from sqlalchemy import create_engine

# Load environment variables
load_dotenv()

# ==================== CONFIG ====================
VIETMAP_API_KEY = os.getenv('VIETMAP_API_KEY', 'YOUR_VIETMAP_KEY')
GEOAPIFY_API_KEY = os.getenv('GEOAPIFY_API_KEY', 'f1f9fa86b35b4087b305c6bb4d6250be')

# Supabase
SUPABASE_URL = os.getenv('SUPABASE_URL', 'YOUR_SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY', 'YOUR_SUPABASE_KEY')
SUPABASE_TABLE = os.getenv('SUPABASE_TABLE', 'locations')

# PostGIS (PostgreSQL)
POSTGIS_HOST = os.getenv('POSTGIS_HOST', 'localhost')
POSTGIS_PORT = os.getenv('POSTGIS_PORT', '5432')
POSTGIS_DB = os.getenv('POSTGIS_DB', 'locations_db')
POSTGIS_USER = os.getenv('POSTGIS_USER', 'postgres')
POSTGIS_PASSWORD = os.getenv('POSTGIS_PASSWORD', 'YOUR_PASSWORD')
POSTGIS_TABLE = os.getenv('POSTGIS_TABLE', 'locations')

# Note: KH√îNG C·∫¶N file GeoJSON!
# Pipeline t·ª± ƒë·ªông t·∫£i ranh gi·ªõi t·ª´ OpenStreetMap b·∫±ng OSMnx

# ==================== STEP 1: GEOCODING ====================

def geocode_vietmap(address):
    """Geocode b·∫±ng Vietmap"""
    url = "https://maps.vietmap.vn/api/search/v3"
    params = {'apikey': VIETMAP_API_KEY, 'text': address}

    try:
        response = requests.get(url, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()

        if data and len(data) > 0:
            return data[0].get('lat'), data[0].get('lng')
    except Exception as e:
        print(f"Vietmap error: {e}")

    return None, None


def geocode_batch(df, address_column='address'):
    """
    Geocode h√†ng lo·∫°t ƒë·ªãa ch·ªâ trong DataFrame

    Args:
        df: DataFrame ch·ª©a ƒë·ªãa ch·ªâ
        address_column: T√™n c·ªôt ch·ª©a ƒë·ªãa ch·ªâ

    Returns:
        DataFrame v·ªõi lat/lon
    """
    print("\nüîç STEP 1: GEOCODING ADDRESSES...")

    lats, lons = [], []

    for idx, row in df.iterrows():
        address = f"{row[address_column]}, Qu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh"
        lat, lon = geocode_vietmap(address)

        lats.append(lat)
        lons.append(lon)

        print(f"  [{idx+1}/{len(df)}] {address[:40]}... ‚Üí ({lat}, {lon})")
        time.sleep(0.5)  # Rate limiting

    df['lat'] = lats
    df['lon'] = lons

    success_rate = df['lat'].notna().sum() / len(df) * 100
    print(f"\n‚úÖ Geocoding complete: {success_rate:.1f}% success rate")

    return df


# ==================== STEP 2: CLEAN DATA ====================

def clean_data(df):
    """
    L√†m s·∫°ch d·ªØ li·ªáu:
    - Remove duplicates
    - Remove rows without lat/lon
    - Standardize columns
    """
    print("\nüßπ STEP 2: CLEANING DATA...")

    initial_count = len(df)

    # Remove null lat/lon
    df = df.dropna(subset=['lat', 'lon'])

    # Remove duplicates (based on lat/lon)
    df = df.drop_duplicates(subset=['lat', 'lon'])

    # Remove invalid coordinates
    df = df[(df['lat'].between(10.5, 11.0)) & (df['lon'].between(106.5, 107.0))]

    print(f"  Initial rows: {initial_count}")
    print(f"  After cleaning: {len(df)}")
    print(f"  Removed: {initial_count - len(df)} rows")

    return df


# ==================== STEP 3: SPATIAL FILTERING ====================

def filter_by_boundary(df, district_query):
    print(f"\nüìç STEP 3: SPATIAL FILTERING...")
    print(f"  Query: {district_query}")

    # 1. T·∫£i ranh gi·ªõi t·ª´ OpenStreetMap (gi·ªëng boundary.py)
    print(f"  üåê Downloading boundary from OpenStreetMap...")
    try:
        gdf_boundary = ox.geocode_to_gdf(district_query)
        print(f"  ‚úÖ Successfully downloaded boundary")
    except Exception as e:
        print(f"  ‚ùå Error downloading from OSM: {e}")
        raise

    # 2. T·∫°o GeoDataFrame t·ª´ c√°c ƒëi·ªÉm (gi·ªëng testDistribution.py)
    print(f"  üìä Creating GeoDataFrame from {len(df)} points...")
    gdf_points = gpd.GeoDataFrame(
        df,
        geometry=gpd.points_from_xy(df.lon, df.lat),
        crs="EPSG:4326"
    )

    # 3. Spatial join: CH·ªà GI·ªÆ ƒëi·ªÉm TRONG ranh gi·ªõi (gi·ªëng testDistribution.py)
    print(f"  üîç Filtering points within boundary...")
    gdf_inside = gpd.sjoin(
        gdf_points,
        gdf_boundary,
        how="inner",        # Ch·ªâ gi·ªØ ƒëi·ªÉm n·∫±m trong
        predicate="within"  # Predicate: ƒëi·ªÉm ph·∫£i n·∫±m HO√ÄN TO√ÄN trong v√πng
    )

    # 4. Th·ªëng k√™
    print(f"  üìä Results:")
    print(f"     Total points: {len(df)}")
    print(f"     ‚úÖ Inside boundary: {len(gdf_inside)}")
    print(f"     ‚ùå Outside boundary (removed): {len(df) - len(gdf_inside)}")

    # 5. Convert v·ªÅ DataFrame th√¥ng th∆∞·ªùng (b·ªè geometry column)
    df_filtered = pd.DataFrame(gdf_inside.drop(columns='geometry'))

    return df_filtered


# ==================== STEP 4: UPLOAD TO SUPABASE ====================

def upload_to_supabase(df, table_name='locations', skip_upload=False):
    """
    Upload d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω l√™n Supabase

    Args:
        skip_upload: N·∫øu True, ch·ªâ preview, kh√¥ng upload th·∫≠t
    """
    print(f"\n‚òÅÔ∏è STEP 4: UPLOADING TO SUPABASE (table: {table_name})...")

    if skip_upload:
        print("  ‚ö†Ô∏è SKIP_UPLOAD=True, ch·ªâ preview, kh√¥ng upload th·∫≠t")
        print(f"  S·∫Ω upload {len(df)} rows v√†o b·∫£ng '{table_name}'")
        return

    # Ki·ªÉm tra c·∫•u h√¨nh
    if SUPABASE_URL == 'YOUR_SUPABASE_URL' or SUPABASE_KEY == 'YOUR_SUPABASE_KEY':
        print("  ‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh Supabase! B·ªè qua b∆∞·ªõc upload.")
        print("  ‚Üí C·∫•u h√¨nh SUPABASE_URL v√† SUPABASE_KEY trong file .env")
        return

    try:
        # Initialize Supabase client
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

        # Convert DataFrame to list of dicts
        records = df.to_dict('records')

        # Upload in batches (Supabase c√≥ limit ~1000 rows/request)
        batch_size = 100
        total_uploaded = 0

        for i in range(0, len(records), batch_size):
            batch = records[i:i+batch_size]

            try:
                response = supabase.table(table_name).insert(batch).execute()
                total_uploaded += len(batch)
                print(f"  Uploaded batch {i//batch_size + 1}: {len(batch)} rows")
            except Exception as e:
                print(f"  ‚ùå Error uploading batch: {e}")

        print(f"\n‚úÖ Upload complete: {total_uploaded}/{len(records)} rows")

    except Exception as e:
        print(f"  ‚ùå L·ªói k·∫øt n·ªëi Supabase: {e}")


def upload_to_postgis(df, table_name='locations', skip_upload=False):
    print(f"\nüó∫Ô∏è STEP 4: UPLOADING TO POSTGIS (table: {table_name})...")

    if skip_upload:
        print("  ‚ö†Ô∏è SKIP_UPLOAD=True, ch·ªâ preview")
        print(f"  S·∫Ω upload {len(df)} rows v·ªõi geometry column")
        return

    # Ki·ªÉm tra c·∫•u h√¨nh
    if POSTGIS_PASSWORD == 'YOUR_PASSWORD':
        print("  ‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh PostGIS! B·ªè qua upload.")
        print("  ‚Üí C·∫•u h√¨nh POSTGIS_* trong file .env")
        return

    try:
        # T·∫°o connection string
        conn_string = f"postgresql://{POSTGIS_USER}:{POSTGIS_PASSWORD}@{POSTGIS_HOST}:{POSTGIS_PORT}/{POSTGIS_DB}"
        engine = create_engine(conn_string)

        print(f"  üì° Connecting to PostGIS: {POSTGIS_HOST}:{POSTGIS_PORT}/{POSTGIS_DB}")

        # T·∫°o GeoDataFrame
        gdf = gpd.GeoDataFrame(
            df,
            geometry=gpd.points_from_xy(df.lon, df.lat),
            crs="EPSG:4326"
        )

        # L∆∞u v√†o PostGIS
        print(f"  üíæ Uploading {len(gdf)} rows...")
        gdf.to_postgis(
            name=table_name,
            con=engine,
            if_exists='replace',  # 'replace' ho·∫∑c 'append'
            index=False
        )

        print(f"  ‚úÖ Uploaded to table '{table_name}'")

        # T·∫°o spatial index (QUAN TR·ªåNG cho performance!)
        print(f"  üîç Creating spatial index...")
        with engine.connect() as conn:
            sql = f"""
            CREATE INDEX IF NOT EXISTS {table_name}_geom_idx
            ON {table_name}
            USING GIST (geometry);
            """
            conn.execute(sql)
            conn.commit()

        print(f"\n‚úÖ PostGIS upload complete!")
        print(f"   Table: {table_name}")
        print(f"   Rows: {len(gdf)}")
        print(f"   Spatial index: ‚úÖ")
        print(f"\nüí° B√¢y gi·ªù c√≥ th·ªÉ d√πng spatial queries:")
        print(f"   - T√¨m ƒëi·ªÉm g·∫ßn nh·∫•t: ST_Distance()")
        print(f"   - T√¨m trong b√°n k√≠nh: ST_DWithin()")
        print(f"   - Xem postgis_utils.py ƒë·ªÉ bi·∫øt th√™m!")

    except Exception as e:
        print(f"  ‚ùå L·ªói k·∫øt n·ªëi PostGIS: {e}")
        import traceback
        traceback.print_exc()


# ==================== MAIN PIPELINE ====================

def run_pipeline(
    input_file,
    district_query,
    output_file=None,
    skip_upload=False,
    upload_to='supabase'
):
    """
    Ch·∫°y to√†n b·ªô pipeline:
    1. Load Excel/CSV
    2. Geocode
    3. Clean
    4. Filter by boundary (OSMnx)
    5. Upload to database

    Args:
        input_file: File CSV/Excel ƒë·∫ßu v√†o
        district_query: Query string cho OSMnx (B·∫ÆT BU·ªòC)
            V√≠ d·ª•: "Qu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh, Vi·ªát Nam"
        output_file: File CSV ƒë·∫ßu ra (optional)
        skip_upload: N·∫øu True, b·ªè qua upload
        upload_to: 'supabase', 'postgis', ho·∫∑c 'both'

    Examples:
        # Upload l√™n Supabase (default)
        run_pipeline('food.csv', 'Qu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh, Vi·ªát Nam')

        # Upload l√™n PostGIS (c√≥ spatial queries!)
        run_pipeline(
            'food.csv',
            'Qu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh, Vi·ªát Nam',
            upload_to='postgis'
        )

        # Upload c·∫£ 2
        run_pipeline(
            'food.csv',
            'Qu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh, Vi·ªát Nam',
            upload_to='both'
        )

        # Ch·ªâ export CSV, kh√¥ng upload
        run_pipeline(
            'food.csv',
            'Qu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh, Vi·ªát Nam',
            output_file='output.csv',
            skip_upload=True
        )
    """
    print("="*60)
    print("üöÄ STARTING DATA PIPELINE")
    print("="*60)

    # Step 0: Load data
    print(f"\nüìÇ STEP 0: LOADING DATA from {input_file}...")
    if input_file.endswith('.xlsx'):
        df = pd.read_excel(input_file)
    else:
        df = pd.read_csv(input_file, encoding='utf-8')

    print(f"  Loaded {len(df)} rows")

    # Step 1: Geocode
    df = geocode_batch(df, address_column='address')

    # Step 2: Clean
    df = clean_data(df)

    # Step 3: Spatial filter (CH·ªà D√ôNG OSMnx)
    df = filter_by_boundary(df, district_query=district_query)

    # Step 4: Upload to database
    if not skip_upload:
        if upload_to == 'supabase':
            upload_to_supabase(df, table_name=SUPABASE_TABLE)
        elif upload_to == 'postgis':
            upload_to_postgis(df, table_name=POSTGIS_TABLE)
        elif upload_to == 'both':
            upload_to_supabase(df, table_name=SUPABASE_TABLE)
            upload_to_postgis(df, table_name=POSTGIS_TABLE)
        else:
            print(f"\n‚ö†Ô∏è Invalid upload_to: {upload_to}. Skipping upload.")
    else:
        print(f"\n‚ö†Ô∏è skip_upload=True, b·ªè qua upload")

    # Optional: Save to CSV
    if output_file:
        df.to_csv(output_file, encoding='utf-8', index=False)
        print(f"\nüíæ Saved processed data to: {output_file}")

    print("\n" + "="*60)
    print("‚úÖ PIPELINE COMPLETE!")
    print("="*60)

    return df


# ==================== EXAMPLE USAGE ====================

if __name__ == "__main__":

    result_df = run_pipeline(
        input_file="food.csv",
        district_query="Qu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh, Vi·ªát Nam",  # ‚Üê B·∫ÆT BU·ªòC
        output_file="quan1_filtered.csv",
        skip_upload=True  # True = kh√¥ng upload Supabase
    )

    print("\n" + "="*60)
    print("üìä FINAL RESULTS")
    print("="*60)
    print(f"Total locations: {len(result_df)}")
    print(f"Output file: quan1_filtered.csv")
    print(f"\nPreview:")
    print(result_df.head())

    # ========================================
    # V√ç D·ª§: X·ª≠ l√Ω nhi·ªÅu qu·∫≠n
    # ========================================
    # districts = [
    #     "Qu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh, Vi·ªát Nam",
    #     "Qu·∫≠n 2, Th√†nh ph·ªë H·ªì Ch√≠ Minh, Vi·ªát Nam",
    #     "Qu·∫≠n 3, Th√†nh ph·ªë H·ªì Ch√≠ Minh, Vi·ªát Nam",
    # ]
    #
    # for i, district in enumerate(districts, 1):
    #     result = run_pipeline(
    #         input_file="food.csv",
    #         district_query=district,
    #         output_file=f"quan{i}_filtered.csv",
    #         skip_upload=True
    #     )
    #     print(f"\nQu·∫≠n {i}: {len(result)} locations")
